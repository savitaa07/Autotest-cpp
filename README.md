# AUTO TEST CPP

 smart, static analysis tool for C++ projects that uses AI to automatically generate test scenarios and predict code coverageâ€”**without compiling or running any tests**. Itâ€™s designed for developers who want deep insights into their codebase using nothing but source files and a local LLM setup.

---

## ğŸ” What It Does



- Scans all `.cpp`, `.cc`, and `.h` files in the `src/` directory.
- Sends each file to an AI model (via Ollama using CodeLlama 7B) for expert-level static analysis.
- Predicts potential test cases and code coverage **without any runtime execution**.
- Stores and reuses analysis using a file-based caching system.
- Generates detailed test reports in Markdown, YAML, and terminal formats.

No compilers. No linkers. No build steps. Just smart analysis.

---

## ğŸ§  System Architecture

![System Architecture](architecture.png)

The system follows a modular, efficient, and build-free pipeline:

1. **Initialization**
   Run `python -m app.main` to start the analysis workflow.

2. **Source Scanning**
   The tool looks inside the `src/` folder (excluding third-party libraries) to find all relevant C++ files.

3. **Cache-First Workflow**

   - If a fileâ€™s analysis exists in `cache/`, itâ€™s reused.
   - Otherwise, the file is sent to the AI model with a structured YAML prompt (`generate_coverage_report.yaml`).

4. **AI Analysis**
   The LLM returns structured JSON containing predicted coverage and test suggestions, which gets cached immediately.

5. **Reporting**
   - A beautiful Markdown summary is created in `reports/`.
   - A machine-readable `coverage_report.yaml` is generated.
   - A terminal summary is printed with key stats.



## ğŸ“‹ Features

- âš¡ **No Build Required** â€“ Purely static analysis, no compilation or test execution.
- ğŸ§  **AI-Powered** â€“ Uses `CodeLlama 7B` model via [Ollama](https://ollama.ai).
- ğŸ“ **Per-File Caching** â€“ Avoid reanalysis of unchanged files.
- ğŸ“Š **Multi-Format Reports** â€“ Outputs Markdown, YAML, and terminal summaries.
- ğŸ”§ **Configurable & Extensible** â€“ Easy model swapping and modular design.

---



## ğŸ“ˆ Performance Optimizations

- ğŸ” **Incremental Analysis** â€“ Only reprocesses changed files.
- ğŸ“¦ **Granular Caching** â€“ One cache file per source.
- ğŸ§© **Persistent Model Server** â€“ Ollama stays running for quick reuse.

---

## âš ï¸ Limitations

- ğŸ§ª **No Actual Test Execution** â€“ This is static analysis only.
- ğŸ¤– **LLM Dependent** â€“ Output quality may vary by model.
- ğŸ’¾ **Large Files May Hit Token Limits** â€“ Especially in LLM input context.

---

