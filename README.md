# CppTestGenAI

CppTestGenAI is a smart, static analysis tool for C++ projects that uses AI to automatically generate test scenarios and predict code coverageâ€”**without compiling or running any tests**. Itâ€™s designed for developers who want deep insights into their codebase using nothing but source files and a local LLM setup.

---

## ğŸ” What It Does

CppTestGenAI analyzes your C++ codebase and:

- Scans all `.cpp`, `.cc`, and `.h` files in the `src/` directory.
- Sends each file to an AI model (via Ollama using CodeLlama 7B) for expert-level static analysis.
- Predicts potential test cases and code coverage **without any runtime execution**.
- Stores and reuses analysis using a file-based caching system.
- Generates detailed test reports in Markdown, YAML, and terminal formats.

No compilers. No linkers. No build steps. Just smart analysis.

---

## ğŸ§  System Architecture

![System Architecture](screenshot/architecture.png)

The system follows a modular, efficient, and build-free pipeline:

1. **Initialization**
   Run `python -m app.main` to start the analysis workflow.

2. **Source Scanning**
   The tool looks inside the `src/` folder (excluding third-party libraries) to find all relevant C++ files.

3. **Cache-First Workflow**

   - If a fileâ€™s analysis exists in `cache/`, itâ€™s reused.
   - Otherwise, the file is sent to the AI model with a structured YAML prompt (`generate_coverage_report.yaml`).

4. **AI Analysis**
   The LLM returns structured JSON containing predicted coverage and test suggestions, which gets cached immediately.

5. **Reporting**
   - A beautiful Markdown summary is created in `reports/`.
   - A machine-readable `coverage_report.yaml` is generated.
   - A terminal summary is printed with key stats.

---

## ğŸ“ Project Structure

```

CppTestGenAI/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py           # Orchestrates everything
â”‚   â”œâ”€â”€ config.py         # Configuration: paths, model, etc.
â”‚   â”œâ”€â”€ llm\_handler.py    # Sends code & prompt to Ollama
â”‚   â”œâ”€â”€ report\_generator.py # Builds human + machine reports
â”‚   â””â”€â”€ ...
â”œâ”€â”€ instructions/
â”‚   â””â”€â”€ generate\_coverage\_report.yaml # Guides the AI analysis
â”œâ”€â”€ src/                  # Your C++ source files go here
â”œâ”€â”€ reports/              # AI-generated Markdown reports
â”œâ”€â”€ cache/                # Cached responses per file
â”œâ”€â”€ screenshots/          # Images for architecture/report
â”œâ”€â”€ requirements.txt      # Python dependencies

```

---

## ğŸ§ª Tested On

This tool was tested using [orgChartApi](https://github.com/keploy/orgChartApi), a real-world C++ project, to verify analysis accuracy and output quality.

## ğŸ“¸ Screenshots

![Coverrage Report](screenshot/coverage_report.png)

---

## ğŸ“‹ Features

- âš¡ **No Build Required** â€“ Purely static analysis, no compilation or test execution.
- ğŸ§  **AI-Powered** â€“ Uses `CodeLlama 7B` model via [Ollama](https://ollama.ai).
- ğŸ“ **Per-File Caching** â€“ Avoid reanalysis of unchanged files.
- ğŸ“Š **Multi-Format Reports** â€“ Outputs Markdown, YAML, and terminal summaries.
- ğŸ”§ **Configurable & Extensible** â€“ Easy model swapping and modular design.

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/aarabii/CppTestGenAI.git
cd CppTestGenAI
```

### 2. Install Ollama & Pull the Model

- Download Ollama from [ollama.ai](https://ollama.ai)
- Then run:

```bash
ollama run codellama:7b
```

### 3. Install Python Requirements

```bash
pip install -r requirements.txt
```

---

## âš™ï¸ Configuration

Open `app/config.py` to change model settings:

```python
MODEL_NAME = "codellama:7b"
```

You can switch to another supported model here, assuming it's available in your Ollama setup.

---

## ğŸ› ï¸ Usage

### Step-by-Step

1. **Place Your C++ Code** in the `src/` directory.

2. **(Optional) Clear Previous Outputs:**

```bash
rm -rf reports/*
rm -rf cache/*
```

3. **Run the Analyzer:**

```bash
python -m app.main
```

---

## ğŸ§  How It Works (Under the Hood)

1. **File Discovery**: Walks `src/` to collect C++ files.
2. **Prompt Formation**: Merges code with YAML-defined instructions.
3. **LLM Request**: Sends prompt to local Ollama server.
4. **Response Caching**: Saves `.log` file per source file in `cache/`.
5. **Report Aggregation**: Combines data into reports using `report_generator.py`.

---

## ğŸ“ˆ Performance Optimizations

- ğŸ” **Incremental Analysis** â€“ Only reprocesses changed files.
- ğŸ“¦ **Granular Caching** â€“ One cache file per source.
- ğŸ§© **Persistent Model Server** â€“ Ollama stays running for quick reuse.

---

## âš ï¸ Limitations

- ğŸ§ª **No Actual Test Execution** â€“ This is static analysis only.
- ğŸ¤– **LLM Dependent** â€“ Output quality may vary by model.
- ğŸ’¾ **Large Files May Hit Token Limits** â€“ Especially in LLM input context.

---

## ğŸ§° Troubleshooting

### Ollama Not Running?

```bash
ollama serve
```

### Model Not Found?

```bash
ollama pull codellama:7b
```

### File Write Issues?

```bash
chmod -R 755 reports cache
```

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a new branch
3. Submit a pull request with your changes

---

## ğŸ“œ License

MIT License â€“ see the [LICENSE](LICENSE) file for full details.

---

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) for local LLM infrastructure
- [CodeLlama](https://github.com/facebookresearch/codellama) by Meta AI
- [orgChartApi](https://github.com/keploy/orgChartApi) for test validation
